pyspark 

fileA_RDD = sc.textFile("file:///home/cloudera/join1_FileA.txt")
fileB_RDD = sc.textFile("file:///home/cloudera/join1_FileB.txt")

fileA_RDD = sc.textFile("input/join1_FileA.txt")
fileB_RDD = sc.textFile("input/join1_FileB.txt")

#Mapper for fileA

def split_fileA(line):
	lines=line.split(',')
	word=lines[0]
	count=lines[1]
	return (word, count)
	
test_line = "able,991"
split_fileA(test_line)

fileA_data = fileA_RDD.map(split_fileA)
fileA_data.collect()
#Result: [(u'able', u'991'), (u'about', u'11'), (u'burger', u'15'), (u'actor', u'22')]

#Mapper for fileB

def split_fileB(line):
	lines=line.split(',')
	count_string=lines[1]
	lines2=lines[0].split()
	word=lines2[1]
	date=lines2[0]
	return (word, date + " " + count_string) 
	
fileB_data = fileB_RDD.map(split_fileB)
fileB_data.collect()

#Result: [(u'able', u'Jan-01 5'), (u'about', u'Feb-02 3'), (u'about', u'Mar-03 8'), (u'able', u'Apr-04 13'), (u'actor', u'Feb-22 3'), (u'burger', u'Feb-23 5'), (u'burger', u'Mar-08 2'), (u'able', u'Dec-15 100')]


fileB_joined_fileA = fileB_data.join(fileA_data)
fileB_joined_fileA.collect()
#Result: [(u'able', (u'Jan-01 5', u'991')), (u'able', (u'Apr-04 13', u'991')), (u'able', (u'Dec-15 100', u'991')), (u'burger', (u'Feb-23 5', u'15')), (u'burger', (u'Mar-08 2', u'15')), (u'about', (u'Feb-02 3', u'11')), (u'about', (u'Mar-03 8', u'11')), (u'actor', (u'Feb-22 3', u'22'))]
